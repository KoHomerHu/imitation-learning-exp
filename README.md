# imitation-learning-exp

An experiment of behavioral cloning and imitation learning on the highway-env.

## To-do List

- [x] Implement a customized PyTorch dataset to load and sample trajectories (by `torch.utils.data.DataLoader`)
- [x] Read the [deep sets](https://arxiv.org/pdf/1703.06114.pdf) paper and implement it
  - I plan to use deep sets instead of the previously implemented [social attention](https://github.com/KoHomerHu/social-attention-exp/tree/main) because deep sets architecture does not need to learn the query and keys. It may be troublesome if the query and keys are entirely different types of entities (e.g. query is multi-modal sensor data where keys are bounding boxes of detected objects).
  - I actually do not know which one is better, it will be good to test whether deep sets or social attention works best in our own use case. So far I empirically see that deep sets reach less loss (hence higher likelihood) faster compared to social attention, which is what I expected (though I haven't tuned the hyperparameters so that the results could be different). Performance-wise, sadly, both architectures only learned to be IDLE for behavioral cloning.
  - Funny thing for the last bullet point: I actually forgot to turn off manual control :), no wonder why they are all IDLE. The result is that social attention seems to work better than deep sets (again, I didn't tune much hyperparameters so you know), specifically it learns to achieve faster speed so that when it turns it would not turn too hurry (I guess so? Please don't trust too much on this observation as I might take it back after more experiments). Imprssively 20 episodes with social attention + behavioral cloning achieve performance higher than double DQN in my previous experiment.
- [x] Collect data of manual control and dump it into a pickle file
  - The distribution of actions in `transition_data.pkl` is almost uniform, this should not happen. The API is not recording the manual control action read from the event handler. `transition_data.pkl` is most likely garbage, I'll just leave it there.
  - `transition_data_mc.pkl` would be the one used for training BC.
  - Need to collect manual control data again and figure out how to store and load transition data by episodes, to allow training with GRU / LSTM.
- [x] Experiment on behavioral cloning
  - As an expert data is expected to have a very unbalanced distribution (e.g. most of the time the ego vehicle may be `IDLE`), and BC on discrete action spaces has no difference to a supervised classification task, we may try using [focal loss](https://arxiv.org/abs/1708.02002v2) to alleviate this problem.
- [ ] Experiment on GAIL
- [ ] Incorporate GRU / LSTM into GAIL

## Thoughts about offline imitation learning

My intention for this experiment is to make the agent not interact with the environment during training (i.e. work in an **offline** setting), given a dataset generated by the expert that only contains tuples in the form of $(s, a, s')$ (i.e. no reward signals). I only found some random papers about "offline imitation learning", and haven't really taken time to read them yet.

I want to figure out how to achieve this (BC is clearly an approach, so I am actually looking for improvements). To me, a direct approach is to use GAIL (or other IRL methods) to learn a reward function $r_\theta(s, a)$, then relabel the dataset of $(s, a, s')$ pairs into $(s, a, r_\theta(s, a), s')$ and conduct Q-learning. I don't know if this will work, so I'll run an experiment first. If it is not working, I may seek to combine GAIL with IQL to see if the issue comes from OOD actions.

## How to use `manual_control.py`

As you can see line 5 of `manual_control.py` imports the `action_listener` method which does not exist in the original code of highway_env:

```from highway_env.envs.common.graphics import action_listener```

I implemented this logic to record the manual control actions as this function is not supported officially.

Specifically, I have added the following parts:

```
current_action = 1
updated = False

def update_action(new_action):
    global current_action, updated
    current_action = new_action
    updated = True

def action_listener():
    global current_action, updated
    if not updated:
        return 1 # if not updated during this iteration, then means no action, i.e. IDLE
    updated = False
    return current_action
```
and modified the `handle_discrete_action_event` method in the `EventHandler` class to:

```
def handle_discrete_action_event(cls, action_type: DiscreteMetaAction, event: pygame.event.EventType) -> None:
    global current_action
    if event.type == pygame.KEYDOWN:
        if event.key == pygame.K_RIGHT and action_type.longitudinal:
            action_type.act(action_type.actions_indexes["FASTER"])
            update_action(3)
        if event.key == pygame.K_LEFT and action_type.longitudinal:
            action_type.act(action_type.actions_indexes["SLOWER"])
            update_action(4)
        if event.key == pygame.K_DOWN and action_type.lateral:
            action_type.act(action_type.actions_indexes["LANE_RIGHT"])
            update_action(2)
        if event.key == pygame.K_UP:
            action_type.act(action_type.actions_indexes["LANE_LEFT"])
            update_action(0)
```
In order to make use of such modification, run the following script to find out where the folder of `highway_env` is:

```
import highway_env
print(highway_env.__file__)
```
Then direct to the `\highway_env\envs\common\` folder and replace the `graphics.py` with the one in my repo.
